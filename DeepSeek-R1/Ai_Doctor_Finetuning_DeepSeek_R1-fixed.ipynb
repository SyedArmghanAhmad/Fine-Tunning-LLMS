{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6cvKXGIDXSo"
   },
   "source": [
    "# **Medical LLM Fine-Tuning Notebook**\n",
    "\n",
    "## **1. Setup & Installation**\n",
    "```python\n",
    "!pip install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install trl==0.14.0 peft==0.14.0 xformers==0.0.28.post3\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install --upgrade datasets huggingface_hub evaluate wandb\n",
    "```\n",
    "**Why?**  \n",
    "- **Unsloth**: Optimizes LoRA fine-tuning for 2x faster training  \n",
    "- **TRL/Peft**: Enables parameter-efficient fine-tuning (LoRA)  \n",
    "- **Torch 2.5.1**: Required for Unsloth compatibility  \n",
    "- **WandB**: Tracks training metrics  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Initialize Model**\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,  # Quantization for memory efficiency\n",
    "    token=hf_token\n",
    ")\n",
    "```\n",
    "**Key Choices:**  \n",
    "- **4-bit Loading**: Reduces VRAM usage by ~75%  \n",
    "- **2048 seq_len**: Accommodates long medical reasoning chains  \n",
    "- **DeepSeek-R1**: Strong base for clinical reasoning  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Pre-Fine-Tuning Test**\n",
    "```python\n",
    "def generate_response(question):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "test_case = \"A 61-year-old woman with urinary incontinence...\"\n",
    "pre_tuning_response = generate_response(test_case)\n",
    "```\n",
    "**Purpose:**  \n",
    "Establishes baseline performance before fine-tuning  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Dataset Preparation**\n",
    "```python\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", split=\"train[:500]\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"\"\"\n",
    "### Clinical Case: {example['Question']}\n",
    "### Analysis: <think>{example['Complex_CoT']}</think>\n",
    "### Diagnosis: {example['Response']}\n",
    "\"\"\"\n",
    "```\n",
    "**Why This Format?**  \n",
    "- **Structured prompts** improve model's clinical reasoning  \n",
    "- **Chain-of-Thought (CoT)**: Forces step-by-step analysis  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. LoRA Configuration**\n",
    "```python\n",
    "model_lora = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0  # Disabled for Unsloth optimization\n",
    ")\n",
    "```\n",
    "**LoRA Parameters Explained:**  \n",
    "- **r=16**: Balance between adaptability and overfitting  \n",
    "- **Target modules**: Where LoRA adapters are injected  \n",
    "- **alpha=16**: Scales LoRA weights (r/alpha ~1:1 ratio recommended)  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Training Setup**\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "    learning_rate=2e-4,  # Optimal for LoRA\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50  # Validate every 50 steps\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_lora,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    formatting_func=format_prompt,\n",
    "    args=training_args\n",
    ")\n",
    "```\n",
    "**Training Strategy:**  \n",
    "- Small batch size → Fits in Colab's free GPU  \n",
    "- Gradient accumulation → Simulates larger batches  \n",
    "- Frequent eval → Catch overfitting early  \n",
    "\n",
    "---\n",
    "\n",
    "## **7. Post-Training Evaluation**\n",
    "```python\n",
    "post_tuning_response = generate_response(test_case)\n",
    "\n",
    "def compare_responses(question, pre, post):\n",
    "    print(f\"PRE: {pre.split('Diagnosis:')[-1]}\")\n",
    "    print(f\"POST: {post.split('Diagnosis:')[-1]}\")\n",
    "```\n",
    "**Expected Improvements:**  \n",
    "1. More accurate diagnoses  \n",
    "2. Better explanation of pathophysiology  \n",
    "3. Reduced hallucinations  \n",
    "\n",
    "---\n",
    "\n",
    "## **8. Gradio Deployment**\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=gr.Textbox(lines=5),\n",
    "    outputs=\"text\",\n",
    "    examples=[test_case]\n",
    ")\n",
    "demo.launch()\n",
    "```\n",
    "**Deployment Notes:**  \n",
    "- Runs locally in Colab  \n",
    "- Add `share=True` for temporary public link  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "1. **LoRA Efficiency**: Achieves good results with only 500 examples  \n",
    "2. **Unsloth Benefits**: 2x faster than standard Peft  \n",
    "3. **Medical Specialization**: Model learns clinical reasoning patterns  \n",
    "4. **Quantization**: Enables fine-tuning on consumer GPUs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install required dependencies\n",
    "!pip install unsloth # install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install trl==0.14.0 peft==0.14.0 xformers==0.0.28.post3\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets huggingface_hub\n",
    "!pip install -qU evaluate rouge_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from huggingface_hub import login\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Configuration\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "max_seq_length = 2048  # For long medical reasoning chains\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=hf_token  # Replace with your HF token\n",
    ")\n",
    "\n",
    "# Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "    prompt = f\"\"\"\n",
    "### Clinical Case:\n",
    "{question}\n",
    "\n",
    "### Step-by-Step Analysis:\n",
    "<think>\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        streamer=streamer,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Case 1: Urinary Incontinence\n",
    "# *A classic clinical reasoning challenge*\n",
    "\n",
    "# %%\n",
    "question = \"\"\"A 61-year-old woman presents with involuntary urine leakage when coughing or sneezing,\n",
    "but no nighttime symptoms. Gynecological exam shows a hypermobile urethra.\n",
    "What is the most likely diagnosis and what would cystometry show?\"\"\"\n",
    "\n",
    "print(\"=== Pre-Fine-Tuning Response ===\")\n",
    "pre_tuning_response = generate_response(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\n",
    "    \"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\",\n",
    "    split=\"train[:500]\",  # First 500 examples\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Split into train/validation\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Formatting function for SINGLE example\n",
    "def format_prompt(example):\n",
    "    return f\"\"\"\n",
    "### Clinical Case:\n",
    "{example['Question']}\n",
    "\n",
    "### Step-by-Step Analysis:\n",
    "<think>\n",
    "{example['Complex_CoT']}\n",
    "</think>\n",
    "\n",
    "### Final Assessment:\n",
    "{example['Response']}\n",
    "\"\"\"\n",
    "\n",
    "# Apply formatting to ENTIRE dataset\n",
    "def preprocess_function(batch):\n",
    "    return {\"text\": [format_prompt({\n",
    "        \"Question\": q,\n",
    "        \"Complex_CoT\": cot,\n",
    "        \"Response\": r\n",
    "    }) for q, cot, r in zip(\n",
    "        batch[\"Question\"],\n",
    "        batch[\"Complex_CoT\"],\n",
    "        batch[\"Response\"]\n",
    "    )]}\n",
    "\n",
    "# Map the formatting across all splits\n",
    "dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names  # Remove original columns\n",
    ")\n",
    "\n",
    "# Preview\n",
    "print(dataset[\"train\"][0][\"text\"])  # Now shows fully formatted example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of re-applying LoRA, create a new LoRA model based on the original model\n",
    "original_model = model  # Store the original model with LoRA\n",
    "\n",
    "# Apply LoRA to the original, non-LoRA model\n",
    "model_lora = FastLanguageModel.get_peft_model(\n",
    "    model=original_model,  # Use the original model instead of original_model.base_model\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3047,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./medical-lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"medical-llm-finetuning\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"medical-deepseek-lora\")\n",
    "tokenizer.save_pretrained(\"medical-deepseek-lora\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model for inference\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"medical-deepseek-lora\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Case 1 Revisited: Urinary Incontinence\n",
    "# Let's compare responses to the same question\n",
    "\n",
    "print(\"=== Post-Fine-Tuning Response ===\")\n",
    "post_tuning_response = generate_response(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"\"\"A 42-year-old IV drug user presents with fever, shortness of breath,\n",
    "and a new murmur. Blood cultures grow gram-positive cocci in clusters.\n",
    "Echocardiography shows vegetations on the tricuspid valve.\n",
    "What is the most likely organism and why is this valve involved?\"\"\"\n",
    "\n",
    "print(\"=== Post-Fine-Tuning Response (New Case) ===\")\n",
    "post_tuning_response2 = generate_response(question2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def compare_responses(question, pre, post):\n",
    "    display(Markdown(f\"\"\"\n",
    "### **Question**:\n",
    "{question}\n",
    "\n",
    "#### **Pre-Fine-Tuning**:\n",
    "```text\n",
    "{pre.split('### Final Assessment:')[-1].strip()}\n",
    "```\n",
    "\n",
    "#### **Post-Fine-Tuning**:\n",
    "```text\n",
    "{post.split('### Final Assessment:')[-1].strip()}\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "compare_responses(question, pre_tuning_response, post_tuning_response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
